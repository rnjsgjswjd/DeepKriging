{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import GPy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../PMdata/covariate0605.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-27f89b16fd6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Read PM2.5 data at June 05, 2019\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../PMdata/covariate0605.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../PMdata/pm25_0605.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../PMdata/covariate0605.csv'"
     ]
    }
   ],
   "source": [
    "## Read PM2.5 data at June 05, 2019\n",
    "df1 = pd.read_csv('../../PMdata/covariate0605.csv')\n",
    "df2 = pd.read_csv('../../PMdata/pm25_0605.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>prec</th>\n",
       "      <th>temp</th>\n",
       "      <th>pres</th>\n",
       "      <th>rh</th>\n",
       "      <th>uwind</th>\n",
       "      <th>vwind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41242</td>\n",
       "      <td>-69.111491</td>\n",
       "      <td>47.386418</td>\n",
       "      <td>0.283942</td>\n",
       "      <td>278.605652</td>\n",
       "      <td>96929.570312</td>\n",
       "      <td>79.589600</td>\n",
       "      <td>4.852976</td>\n",
       "      <td>1.651037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41243</td>\n",
       "      <td>-68.736976</td>\n",
       "      <td>47.244262</td>\n",
       "      <td>0.276129</td>\n",
       "      <td>278.605652</td>\n",
       "      <td>97242.070312</td>\n",
       "      <td>79.589600</td>\n",
       "      <td>4.470163</td>\n",
       "      <td>1.646155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41244</td>\n",
       "      <td>-68.364554</td>\n",
       "      <td>47.100850</td>\n",
       "      <td>0.276129</td>\n",
       "      <td>279.753113</td>\n",
       "      <td>98692.070312</td>\n",
       "      <td>75.652100</td>\n",
       "      <td>4.477976</td>\n",
       "      <td>1.624670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41245</td>\n",
       "      <td>-67.994227</td>\n",
       "      <td>46.956192</td>\n",
       "      <td>0.604254</td>\n",
       "      <td>280.398621</td>\n",
       "      <td>98792.070312</td>\n",
       "      <td>77.417725</td>\n",
       "      <td>4.096140</td>\n",
       "      <td>1.615881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41589</td>\n",
       "      <td>-69.318835</td>\n",
       "      <td>47.131182</td>\n",
       "      <td>0.174567</td>\n",
       "      <td>278.424988</td>\n",
       "      <td>96567.070312</td>\n",
       "      <td>77.456787</td>\n",
       "      <td>5.140085</td>\n",
       "      <td>1.481116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       long        lat      prec        temp          pres  \\\n",
       "0       41242 -69.111491  47.386418  0.283942  278.605652  96929.570312   \n",
       "1       41243 -68.736976  47.244262  0.276129  278.605652  97242.070312   \n",
       "2       41244 -68.364554  47.100850  0.276129  279.753113  98692.070312   \n",
       "3       41245 -67.994227  46.956192  0.604254  280.398621  98792.070312   \n",
       "4       41589 -69.318835  47.131182  0.174567  278.424988  96567.070312   \n",
       "\n",
       "          rh     uwind     vwind  \n",
       "0  79.589600  4.852976  1.651037  \n",
       "1  79.589600  4.470163  1.646155  \n",
       "2  75.652100  4.477976  1.624670  \n",
       "3  77.417725  4.096140  1.615881  \n",
       "4  77.456787  5.140085  1.481116  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>PM25</th>\n",
       "      <th>PM_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-80.482778</td>\n",
       "      <td>25.471944</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-80.215556</td>\n",
       "      <td>25.794222</td>\n",
       "      <td>4.929167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-97.493830</td>\n",
       "      <td>25.892518</td>\n",
       "      <td>8.213636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-80.326389</td>\n",
       "      <td>25.941944</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-80.256944</td>\n",
       "      <td>26.053889</td>\n",
       "      <td>4.992754</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Longitude   Latitude      PM25  PM_class\n",
       "0           1 -80.482778  25.471944  4.400000         0\n",
       "1           2 -80.215556  25.794222  4.929167         0\n",
       "2           3 -97.493830  25.892518  8.213636         0\n",
       "3           4 -80.326389  25.941944  4.100000         0\n",
       "4           5 -80.256944  26.053889  4.992754         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7706, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates = df1.values[:,3:]\n",
    "covariates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aqs_lonlat=df2.values[:,[1,2]]\n",
    "aqs_lonlat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair the long and lat based on the nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "near = df1.values[:,[1,2]]\n",
    "tree = spatial.KDTree(list(zip(near[:,0].ravel(), near[:,1].ravel())))\n",
    "tree.data\n",
    "idx = tree.query(aqs_lonlat)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_new = df2.assign(neighbor = idx)\n",
    "df_pm25 = df2_new.groupby('neighbor')['PM25'].mean()\n",
    "df_pm25_class = pd.cut(df_pm25,bins=[0,12.1,35.5],labels=[\"0\",\"1\"])\n",
    "idx_new = df_pm25.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = df_pm25.values\n",
    "pm25_class = np.array(df_pm25_class.values,dtype=int)\n",
    "z = pm25[:,None]\n",
    "z_class = pm25_class[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = df1.values[:,1]\n",
    "lat = df1.values[:,2]\n",
    "normalized_lon = (lon-min(lon))/(max(lon)-min(lon))\n",
    "normalized_lat = (lat-min(lat))/(max(lat)-min(lat))\n",
    "N = lon.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis = [10**2,19**2,37**2]\n",
    "knots_1dx = [np.linspace(0,1,np.sqrt(i)) for i in num_basis]\n",
    "knots_1dy = [np.linspace(0,1,np.sqrt(i)) for i in num_basis]\n",
    "##Wendland kernel\n",
    "basis_size = 0\n",
    "phi = np.zeros((N, sum(num_basis)))\n",
    "for res in range(len(num_basis)):\n",
    "    theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "    knots_x, knots_y = np.meshgrid(knots_1dx[res],knots_1dy[res])\n",
    "    knots = np.column_stack((knots_x.flatten(),knots_y.flatten()))\n",
    "    for i in range(num_basis[res]):\n",
    "        d = np.linalg.norm(np.vstack((normalized_lon,normalized_lat)).T-knots[i,:],axis=1)/theta\n",
    "        for j in range(len(d)):\n",
    "            if d[j] >= 0 and d[j] <= 1:\n",
    "                phi[j,i + basis_size] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "            else:\n",
    "                phi[j,i + basis_size] = 0\n",
    "    basis_size = basis_size + num_basis[res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Romove the all-zero columns\n",
    "idx_zero = np.array([], dtype=int)\n",
    "for i in range(phi.shape[1]):\n",
    "    if sum(phi[:,i]!=0)==0:\n",
    "        idx_zero = np.append(idx_zero,int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7706, 1830)\n",
      "(7706, 1486)\n"
     ]
    }
   ],
   "source": [
    "phi_reduce = np.delete(phi,idx_zero,1)\n",
    "print(phi.shape)\n",
    "print(phi_reduce.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_obs = phi_reduce[idx_new,:]\n",
    "s_obs = np.vstack((normalized_lon[idx_new],normalized_lat[idx_new])).T\n",
    "X = covariates[idx_new,:]\n",
    "normalized_X = X\n",
    "for i in range(X.shape[1]):\n",
    "    normalized_X[:,i] = (X[:,i]-min(X[:,i]))/(max(X[:,i])-min(X[:,i]))\n",
    "N_obs = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndYgW7hqNRlOH/D/YyhrJqPeHjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107ac9160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot of the circles dataset with points colored by class\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "# select indices of points with each class label\n",
    "for i in range(2):\n",
    "\tsamples_ix = where(z_class[:,0] == i)\n",
    "\tpyplot.scatter(s_obs[samples_ix, 0], s_obs[samples_ix, 1], label=str(i))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the prepocessed data for Kriging\n",
    "z_log = np.log(z)\n",
    "kriging = np.concatenate((s_obs, normalized_X, z, z_class,z_log),axis=1)\n",
    "traindf=pd.DataFrame(kriging)\n",
    "traindf.to_csv('../../PMdata/kriging.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepKriging and baseline DNN performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(model, X_train, y_train, X_valid, y_valid, data_type):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    if data_type == 'continuous':\n",
    "        model.compile(optimizer='adam'\n",
    "                      , loss='mse'\n",
    "                      , metrics=['mse','mae'])\n",
    "    if data_type == 'discrete':\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=0)\n",
    "    return history\n",
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return results\n",
    "    \n",
    "def optimal_epoch(model_hist):\n",
    "    '''\n",
    "    Function to return the epoch number where the validation loss is\n",
    "    at its minimum\n",
    "    \n",
    "    Parameters:\n",
    "        model_hist : training history of model\n",
    "    Output:\n",
    "        epoch number with minimum validation loss\n",
    "    '''\n",
    "    min_epoch = np.argmin(model_hist.history['val_loss']) + 1\n",
    "    return min_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/liy0h/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/liy0h/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# DeepKriging model for continuous data\n",
    "p = covariates.shape[1] + phi_reduce.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim = p,  kernel_initializer='he_uniform', activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepKriging model for categorical data\n",
    "model_class = Sequential()\n",
    "model_class.add(Dense(100, input_dim = p,  kernel_initializer='he_uniform', activation='relu'))\n",
    "model_class.add(Dropout(rate=0.5))\n",
    "model_class.add(BatchNormalization())\n",
    "model_class.add(Dense(100, activation='relu'))\n",
    "model_class.add(Dropout(rate=0.5))\n",
    "model_class.add(Dense(100, activation='relu'))\n",
    "model_class.add(BatchNormalization())\n",
    "model_class.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline DNN only with covariates and coordinates\n",
    "p_base = covariates.shape[1] + s_obs.shape[1]\n",
    "# Neural network\n",
    "model_base = Sequential()\n",
    "model_base.add(Dense(100, input_dim=p_base,  kernel_initializer='he_uniform', activation='relu'))\n",
    "model_base.add(Dropout(rate=0.5))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Dense(100, activation='relu'))\n",
    "model_base.add(Dropout(rate=0.5))\n",
    "model_base.add(Dense(100, activation='relu'))\n",
    "model_base.add(BatchNormalization())\n",
    "model_base.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline DNN for classification\n",
    "model_base_class = Sequential()\n",
    "model_base_class.add(Dense(100, input_dim=p_base,  kernel_initializer='he_uniform', activation='relu'))\n",
    "model_base_class.add(Dropout(rate=0.5))\n",
    "model_base_class.add(BatchNormalization())\n",
    "model_base_class.add(Dense(100, activation='relu'))\n",
    "model_base_class.add(Dropout(rate=0.5))\n",
    "model_base_class.add(Dense(100, activation='relu'))\n",
    "model_base_class.add(BatchNormalization())\n",
    "model_base_class.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 64  # Size of the batches used in the mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.76129097e-01, 2.79753113e+02, 9.86920703e+04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.90191597e-01, 2.79539246e+02, 9.85295703e+04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.69158557e-03, 2.78927917e+02, 9.98920703e+04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [1.00105038e+01, 2.95668152e+02, 1.01179570e+05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.93316597e-01, 2.93506042e+02, 1.00979570e+05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [8.73706627e+00, 2.92403503e+02, 1.01292070e+05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "The performance of DeepKriging: MSE = 1.6296970043026033, MAE = 0.8693870306015015\n",
      "The performance of classical DNN: MSE = 3.660423450782651, MAE = 1.4688420315257837\n",
      "The performance of DeepKriging: accuracy = 0.9508196721311475\n",
      "The performance of classical DNN: accuracy = 0.9508196721311475\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "The performance of DeepKriging: MSE = 1.2945499537421055, MAE = 0.8958792100187207\n",
      "The performance of classical DNN: MSE = 3.535540807442587, MAE = 1.4687806406959159\n",
      "The performance of DeepKriging: accuracy = 0.9508196721311475\n",
      "The performance of classical DNN: accuracy = 0.9180327976336244\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "The performance of DeepKriging: MSE = 1.89918517284706, MAE = 0.9561366438865662\n",
      "The performance of classical DNN: MSE = 3.203148103151165, MAE = 1.4454119361814906\n",
      "The performance of DeepKriging: accuracy = 0.9344262207140688\n",
      "The performance of classical DNN: accuracy = 0.8360655845188704\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "The performance of DeepKriging: MSE = 1.7020849556219382, MAE = 0.9200828182892721\n",
      "The performance of classical DNN: MSE = 4.308605538039911, MAE = 1.5220062830409065\n",
      "The performance of DeepKriging: accuracy = 0.9344262295081968\n",
      "The performance of classical DNN: accuracy = 0.9016393442622951\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "The performance of DeepKriging: MSE = 1.2021335204442343, MAE = 0.8806833306948344\n",
      "The performance of classical DNN: MSE = 3.652622620264689, MAE = 1.398123033841451\n",
      "The performance of DeepKriging: accuracy = 1.0\n",
      "The performance of classical DNN: accuracy = 0.900000007947286\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "The performance of DeepKriging: MSE = 1.2118785699208579, MAE = 0.834563946723938\n",
      "The performance of classical DNN: MSE = 3.4818862597147624, MAE = 1.4840887705485026\n",
      "The performance of DeepKriging: accuracy = 0.950000007947286\n",
      "The performance of classical DNN: accuracy = 0.8666666547457377\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "The performance of DeepKriging: MSE = 1.5648294766743978, MAE = 0.8874627669652303\n",
      "The performance of classical DNN: MSE = 2.979759629567464, MAE = 1.290592122077942\n",
      "The performance of DeepKriging: accuracy = 0.899999996026357\n",
      "The performance of classical DNN: accuracy = 0.800000003973643\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "The performance of DeepKriging: MSE = 2.5505664507548014, MAE = 0.9255451997121175\n",
      "The performance of classical DNN: MSE = 2.9916441599527994, MAE = 1.3403977870941162\n",
      "The performance of DeepKriging: accuracy = 0.950000007947286\n",
      "The performance of classical DNN: accuracy = 0.95\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "The performance of DeepKriging: MSE = 2.6006882349650065, MAE = 1.0915081659952799\n",
      "The performance of classical DNN: MSE = 6.0217059771219885, MAE = 1.850953475634257\n",
      "The performance of DeepKriging: accuracy = 0.9833333333333333\n",
      "The performance of classical DNN: accuracy = 0.8833333293596903\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "The performance of DeepKriging: MSE = 0.6652140339215596, MAE = 0.6562575578689576\n",
      "The performance of classical DNN: MSE = 2.4804024855295816, MAE = 1.2143369595209756\n",
      "The performance of DeepKriging: accuracy = 0.9666666746139526\n",
      "The performance of classical DNN: accuracy = 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state = 123)\n",
    "fold_no = 1\n",
    "inputs = np.hstack((normalized_X,phi_obs))\n",
    "inputs_base = np.hstack((normalized_X,s_obs))\n",
    "targets = z\n",
    "targets_class = z_class\n",
    "mse_per_fold = []\n",
    "mse_per_fold_base = []\n",
    "mae_per_fold = []\n",
    "mae_per_fold_base = []\n",
    "acc_per_fold = []\n",
    "acc_per_fold_base = []\n",
    "for train_idx, test_idx in kfold.split(inputs, targets):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    history = deep_model(model, inputs[train_idx], targets[train_idx,0:1]\\\n",
    "                              , inputs[test_idx], targets[test_idx,0:1],'continuous')\n",
    "    history_base = deep_model(model_base, inputs_base[train_idx], targets[train_idx,0:1]\\\n",
    "                              , inputs_base[test_idx], targets[test_idx,0:1],'continuous')\n",
    "    ## Classification\n",
    "    history_class = deep_model(model_class, inputs[train_idx], targets_class[train_idx,0:1]\\\n",
    "                              , inputs[test_idx], targets_class[test_idx,0:1],'discrete')\n",
    "    history_base_class = deep_model(model_base_class, inputs_base[train_idx], targets_class[train_idx,0:1]\\\n",
    "                              , inputs_base[test_idx], targets_class[test_idx,0:1],'discrete')\n",
    "    model_optim = 200#optimal_epoch(history)\n",
    "    model_optim_base = 200#optimal_epoch(history_base)\n",
    "    result = test_model(model, inputs[train_idx], targets[train_idx,0:1], inputs[test_idx]\\\n",
    "                        , targets[test_idx,0:1], model_optim)\n",
    "    result_base = test_model(model_base, inputs_base[train_idx], targets[train_idx,0:1], inputs_base[test_idx]\\\n",
    "                             , targets[test_idx,0:1], model_optim_base)\n",
    "    scores = result\n",
    "    scores_base = result_base\n",
    "    print(f'The performance of DeepKriging: MSE = {scores[1]}, MAE = {scores[2]}')\n",
    "    print(f'The performance of classical DNN: MSE = {scores_base[1]}, MAE = {scores_base[2]}')\n",
    "    model_optim_class = 200#optimal_epoch(history_class)\n",
    "    model_optim_base_class = 200#optimal_epoch(history_base_class)\n",
    "    result_class = test_model(model_class, inputs[train_idx], targets_class[train_idx,0:1], inputs[test_idx]\\\n",
    "                        , targets_class[test_idx,0:1], model_optim_class)\n",
    "    result_base_class = test_model(model_base_class, inputs_base[train_idx], targets_class[train_idx,0:1], inputs_base[test_idx]\\\n",
    "                             , targets_class[test_idx,0:1], model_optim_base_class)\n",
    "    scores_class = result_class\n",
    "    scores_base_class = result_base_class\n",
    "    print(f'The performance of DeepKriging: accuracy = {scores_class[1]}')\n",
    "    print(f'The performance of classical DNN: accuracy = {result_base_class[1]}')\n",
    "    fold_no = fold_no + 1\n",
    "    acc_per_fold.append(scores_class[1])\n",
    "    acc_per_fold_base.append(scores_base_class[1])\n",
    "    mse_per_fold.append(scores[1])\n",
    "    mse_per_fold_base.append(scores_base[1])\n",
    "    mae_per_fold.append(scores[2])\n",
    "    mae_per_fold_base.append(scores_base[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6320827373194564\n",
      "0.5720815498971454\n",
      "3.6315739031567595\n",
      "0.9245065429165812\n",
      "0.8917506670756419\n",
      "0.10265416124373837\n",
      "1.4483533040161343\n",
      "0.1623157763284635\n",
      "0.9520491814352775\n",
      "0.026140632660667334\n",
      "0.8956557382651364\n",
      "0.048164592313833354\n"
     ]
    }
   ],
   "source": [
    "##Summerize the results\n",
    "print(np.mean(mse_per_fold))\n",
    "print(np.std(mse_per_fold))\n",
    "print(np.mean(mse_per_fold_base))\n",
    "print(np.std(mse_per_fold_base))\n",
    "print(np.mean(mae_per_fold))\n",
    "print(np.std(mae_per_fold))\n",
    "print(np.mean(mae_per_fold_base))\n",
    "print(np.std(mae_per_fold_base))\n",
    "print(np.mean(acc_per_fold))\n",
    "print(np.std(acc_per_fold))\n",
    "print(np.mean(acc_per_fold_base))\n",
    "print(np.std(acc_per_fold_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the same test index for Kriging\n",
    "train_idx_combine = []\n",
    "test_idx_combine = []\n",
    "for train_idx, test_idx in kfold.split(inputs, targets):\n",
    "    train_idx_combine.append(train_idx)\n",
    "    test_idx_combine.append(test_idx)  \n",
    "test_df = pd.DataFrame(test_idx_combine).T\n",
    "test_df.to_csv('../../PMdata/test_idx.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The case without covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepKriging model for continuous data\n",
    "p2 =  phi_reduce.shape[1]\n",
    "model_nx = Sequential()\n",
    "model_nx.add(Dense(100, input_dim = p2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "model_nx.add(Dropout(rate=0.5))\n",
    "model_nx.add(BatchNormalization())\n",
    "model_nx.add(Dense(100, activation='relu'))\n",
    "model_nx.add(Dropout(rate=0.5))\n",
    "model_nx.add(Dense(100, activation='relu'))\n",
    "model_nx.add(BatchNormalization())\n",
    "model_nx.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepKriging model for categorical data\n",
    "model_class_nx = Sequential()\n",
    "model_class_nx.add(Dense(100, input_dim = p2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "model_class_nx.add(Dropout(rate=0.5))\n",
    "model_class_nx.add(BatchNormalization())\n",
    "model_class_nx.add(Dense(100, activation='relu'))\n",
    "model_class_nx.add(Dropout(rate=0.5))\n",
    "model_class_nx.add(Dense(100, activation='relu'))\n",
    "model_class_nx.add(BatchNormalization())\n",
    "model_class_nx.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "The performance of DeepKriging without X: MSE = 2.084441997965828, MAE = 1.0120577304089655\n",
      "The performance of DeepKriging without X: accuracy = 0.9672131059599705\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "The performance of DeepKriging without X: MSE = 1.6166401261188945, MAE = 0.9455184682470853\n",
      "The performance of DeepKriging without X: accuracy = 0.9508196721311475\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "The performance of DeepKriging without X: MSE = 1.0659916225026866, MAE = 0.7141352696496932\n",
      "The performance of DeepKriging without X: accuracy = 0.9672131147540983\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "The performance of DeepKriging without X: MSE = 1.5375527045765862, MAE = 0.8813984159563408\n",
      "The performance of DeepKriging without X: accuracy = 0.9672131147540983\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "The performance of DeepKriging without X: MSE = 2.0205376148223877, MAE = 1.0472495873769125\n",
      "The performance of DeepKriging without X: accuracy = 0.9833333333333333\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "The performance of DeepKriging without X: MSE = 1.1234728495279949, MAE = 0.8217819174130757\n",
      "The performance of DeepKriging without X: accuracy = 0.9333333412806193\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "The performance of DeepKriging without X: MSE = 0.8016857822736104, MAE = 0.7359609643618266\n",
      "The performance of DeepKriging without X: accuracy = 0.9166666547457377\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "The performance of DeepKriging without X: MSE = 0.8026497999827067, MAE = 0.7297665397326152\n",
      "The performance of DeepKriging without X: accuracy = 0.9666666666666667\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "The performance of DeepKriging without X: MSE = 1.9144471883773804, MAE = 1.011347508430481\n",
      "The performance of DeepKriging without X: accuracy = 0.9666666666666667\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "The performance of DeepKriging without X: MSE = 0.8797891139984131, MAE = 0.7498628656069438\n",
      "The performance of DeepKriging without X: accuracy = 0.950000007947286\n"
     ]
    }
   ],
   "source": [
    "inputs = phi_obs\n",
    "inputs_base = s_obs\n",
    "targets = z\n",
    "targets_class = z_class\n",
    "mse_per_fold_nx = []\n",
    "mae_per_fold_nx = []\n",
    "acc_per_fold_nx = []\n",
    "fold_no = 1\n",
    "for train_idx, test_idx in kfold.split(inputs, targets):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    history_nx = deep_model(model_nx, inputs[train_idx], targets[train_idx,0:1]\\\n",
    "                              , inputs[test_idx], targets[test_idx,0:1],'continuous')\n",
    "    ## Classification\n",
    "    history_class_nx = deep_model(model_class_nx, inputs[train_idx], targets_class[train_idx,0:1]\\\n",
    "                              , inputs[test_idx], targets_class[test_idx,0:1],'discrete')\n",
    "    model_optim_nx = 200#optimal_epoch(history_nx)\n",
    "    result_nx = test_model(model_nx, inputs[train_idx], targets[train_idx,0:1], inputs[test_idx]\\\n",
    "                        , targets[test_idx,0:1], model_optim_nx)\n",
    "    scores_nx = result_nx\n",
    "    print(f'The performance of DeepKriging without X: MSE = {scores_nx[1]}, MAE = {scores_nx[2]}')\n",
    "    model_optim_class_nx = 200#optimal_epoch(history_class_nx)\n",
    "    result_class_nx = test_model(model_class_nx, inputs[train_idx], targets_class[train_idx,0:1], inputs[test_idx]\\\n",
    "                        , targets_class[test_idx,0:1], model_optim_class_nx)\n",
    "    scores_class_nx = result_class_nx\n",
    "    print(f'The performance of DeepKriging without X: accuracy = {scores_class_nx[1]}')\n",
    "    fold_no = fold_no + 1\n",
    "    acc_per_fold_nx.append(scores_class_nx[1])\n",
    "    mse_per_fold_nx.append(scores_nx[1])\n",
    "    mae_per_fold_nx.append(scores_nx[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3847208800146489\n",
      "0.48529972769614593\n",
      "0.8649079267183941\n",
      "0.12483357686634979\n",
      "0.9569125678239624\n",
      "0.01862059522258444\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(mse_per_fold_nx))\n",
    "print(np.std(mse_per_fold_nx))\n",
    "print(np.mean(mae_per_fold_nx))\n",
    "print(np.std(mae_per_fold_nx))\n",
    "print(np.mean(acc_per_fold_nx))\n",
    "print(np.std(acc_per_fold_nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.83941597e-01,  2.78605652e+02,  9.69295703e+04,\n",
       "         7.95895996e+01,  4.85297585e+00,  1.65103745e+00],\n",
       "       [ 2.76129097e-01,  2.78605652e+02,  9.72420703e+04,\n",
       "         7.95895996e+01,  4.47016335e+00,  1.64615464e+00],\n",
       "       [ 2.76129097e-01,  2.79753113e+02,  9.86920703e+04,\n",
       "         7.56520996e+01,  4.47797585e+00,  1.62467027e+00],\n",
       "       ...,\n",
       "       [ 3.93316597e-01,  2.93506042e+02,  1.00979570e+05,\n",
       "         7.11989822e+01,  1.48481190e+00, -3.00521255e+00],\n",
       "       [ 2.06519151e+00,  2.92847839e+02,  1.01029570e+05,\n",
       "         7.33864746e+01,  1.86274159e+00, -2.99056411e+00],\n",
       "       [ 8.73706627e+00,  2.92403503e+02,  1.01292070e+05,\n",
       "         7.79099121e+01,  1.79731190e+00, -3.06185317e+00]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.hstack((normalized_X,phi_obs))\n",
    "targets = z\n",
    "history = deep_model(model, inputs, targets[:,0:1], inputs, targets[:,0:1],'continuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = covariates\n",
    "normalized_X_pred = X_pred\n",
    "for i in range(X_pred.shape[1]):\n",
    "    normalized_X_pred[:,i] = (X_pred[:,i]-min(X[:,i]))/(max(X[:,i])-min(X[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RBF_pred = np.hstack((normalized_X_pred,phi_reduce))\n",
    "PM25_pred = model.predict(X_RBF_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.vstack((normalized_lon,normalized_lat)).T\n",
    "PM25_pred_combine = np.concatenate((s,normalized_X_pred,PM25_pred),axis=1)\n",
    "PM25_pred_df=pd.DataFrame(PM25_pred_combine)\n",
    "PM25_pred_df.to_csv('../../PMdata/PM25_pred_0605.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from dcdr.deep_hist import Binning_CDF\n",
    "from scipy.stats import iqr\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib;matplotlib.rcParams['figure.figsize'] = (8,6)\n",
    "init_seed=1\n",
    "num_cut = int((max(z)-min(z))*N**(1/3)/(2*iqr(z)))\n",
    "hidden_list = [100,100,100]\n",
    "dropout_list = [0.5,0.5,0.5]\n",
    "histogram_bin = 'random'\n",
    "loss_model = 'multi-binary'\n",
    "seeding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 33ms/step - loss: 0.5635 - acc: 0.7238 - val_loss: 0.6200 - val_acc: 0.8275\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.4797 - acc: 0.7725 - val_loss: 0.5747 - val_acc: 0.8420\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.4475 - acc: 0.7916 - val_loss: 0.5004 - val_acc: 0.8593\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.3893 - acc: 0.8220 - val_loss: 0.4857 - val_acc: 0.8818\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.3648 - acc: 0.8393 - val_loss: 0.4791 - val_acc: 0.8833\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 295us/step - loss: 0.3696 - acc: 0.8382 - val_loss: 0.4531 - val_acc: 0.8889\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.3287 - acc: 0.8660 - val_loss: 0.4437 - val_acc: 0.8901\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.3123 - acc: 0.8660 - val_loss: 0.4299 - val_acc: 0.8929\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.2984 - acc: 0.8703 - val_loss: 0.4233 - val_acc: 0.8948\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.2702 - acc: 0.8913 - val_loss: 0.4099 - val_acc: 0.8954\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.2696 - acc: 0.8869 - val_loss: 0.4020 - val_acc: 0.8938\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.2568 - acc: 0.8962 - val_loss: 0.4016 - val_acc: 0.8941\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.2552 - acc: 0.8904 - val_loss: 0.3981 - val_acc: 0.8978\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 293us/step - loss: 0.2302 - acc: 0.9094 - val_loss: 0.3881 - val_acc: 0.9019\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.2359 - acc: 0.9023 - val_loss: 0.3905 - val_acc: 0.9025\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.2343 - acc: 0.9069 - val_loss: 0.4122 - val_acc: 0.9022\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.2193 - acc: 0.9117 - val_loss: 0.4222 - val_acc: 0.9009\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 293us/step - loss: 0.2192 - acc: 0.9093 - val_loss: 0.4347 - val_acc: 0.9028\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.2223 - acc: 0.9085 - val_loss: 0.4426 - val_acc: 0.9019\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.2066 - acc: 0.9145 - val_loss: 0.4452 - val_acc: 0.9028\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.2077 - acc: 0.9144 - val_loss: 0.4396 - val_acc: 0.9043\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1951 - acc: 0.9217 - val_loss: 0.4446 - val_acc: 0.9040\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1961 - acc: 0.9247 - val_loss: 0.4454 - val_acc: 0.9031\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1853 - acc: 0.9268 - val_loss: 0.4417 - val_acc: 0.9043\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1799 - acc: 0.9303 - val_loss: 0.4408 - val_acc: 0.9031\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.1801 - acc: 0.9314 - val_loss: 0.4512 - val_acc: 0.9028\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1807 - acc: 0.9253 - val_loss: 0.4531 - val_acc: 0.9037\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 276us/step - loss: 0.1792 - acc: 0.9253 - val_loss: 0.4527 - val_acc: 0.9049\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.1695 - acc: 0.9324 - val_loss: 0.4477 - val_acc: 0.9071\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.1658 - acc: 0.9331 - val_loss: 0.4463 - val_acc: 0.9077\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.1648 - acc: 0.9352 - val_loss: 0.4438 - val_acc: 0.9086\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.1536 - acc: 0.9379 - val_loss: 0.4439 - val_acc: 0.9099\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 291us/step - loss: 0.1648 - acc: 0.9307 - val_loss: 0.4455 - val_acc: 0.9102\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 273us/step - loss: 0.1550 - acc: 0.9412 - val_loss: 0.4590 - val_acc: 0.9071\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1550 - acc: 0.9395 - val_loss: 0.4623 - val_acc: 0.9059\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1565 - acc: 0.9364 - val_loss: 0.4554 - val_acc: 0.9068\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1508 - acc: 0.9400 - val_loss: 0.4552 - val_acc: 0.9059\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1524 - acc: 0.9386 - val_loss: 0.4480 - val_acc: 0.9052\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1429 - acc: 0.9415 - val_loss: 0.4428 - val_acc: 0.9083\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1450 - acc: 0.9413 - val_loss: 0.4382 - val_acc: 0.9090\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1513 - acc: 0.9384 - val_loss: 0.4367 - val_acc: 0.9093\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1441 - acc: 0.9404 - val_loss: 0.4351 - val_acc: 0.9086\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.1487 - acc: 0.9419 - val_loss: 0.4357 - val_acc: 0.9099\n",
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.1407 - acc: 0.9437 - val_loss: 0.4361 - val_acc: 0.9105\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1589 - acc: 0.9367 - val_loss: 0.4366 - val_acc: 0.9086\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1407 - acc: 0.9431 - val_loss: 0.4376 - val_acc: 0.9090\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1487 - acc: 0.9452 - val_loss: 0.4392 - val_acc: 0.9096\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1325 - acc: 0.9476 - val_loss: 0.4412 - val_acc: 0.9099\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1401 - acc: 0.9435 - val_loss: 0.4442 - val_acc: 0.9096\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1469 - acc: 0.9440 - val_loss: 0.4460 - val_acc: 0.9102\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1347 - acc: 0.9462 - val_loss: 0.4430 - val_acc: 0.9105\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1341 - acc: 0.9457 - val_loss: 0.4395 - val_acc: 0.9111\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.1489 - acc: 0.9371 - val_loss: 0.4440 - val_acc: 0.9108\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 0s 349us/step - loss: 0.1331 - acc: 0.9452 - val_loss: 0.4428 - val_acc: 0.9108\n",
      "Epoch 55/500\n",
      "484/484 [==============================] - 0s 336us/step - loss: 0.1435 - acc: 0.9434 - val_loss: 0.4441 - val_acc: 0.9102\n",
      "Epoch 56/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1381 - acc: 0.9463 - val_loss: 0.4477 - val_acc: 0.9102\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 0s 360us/step - loss: 0.1437 - acc: 0.9423 - val_loss: 0.4506 - val_acc: 0.9099\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 0s 362us/step - loss: 0.1480 - acc: 0.9433 - val_loss: 0.4535 - val_acc: 0.9090\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 0s 341us/step - loss: 0.1330 - acc: 0.9480 - val_loss: 0.4536 - val_acc: 0.9090\n",
      "Epoch 60/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1486 - acc: 0.9428 - val_loss: 0.4545 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1412 - acc: 0.9439 - val_loss: 0.4508 - val_acc: 0.9086\n",
      "Epoch 62/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.1403 - acc: 0.9460 - val_loss: 0.4548 - val_acc: 0.9086\n",
      "Epoch 63/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1364 - acc: 0.9449 - val_loss: 0.4548 - val_acc: 0.9086\n",
      "Epoch 64/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1458 - acc: 0.9410 - val_loss: 0.4576 - val_acc: 0.9086\n",
      "Epoch 65/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1446 - acc: 0.9414 - val_loss: 0.4571 - val_acc: 0.9083\n",
      "Epoch 66/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1394 - acc: 0.9448 - val_loss: 0.4544 - val_acc: 0.9086\n",
      "Epoch 67/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1488 - acc: 0.9432 - val_loss: 0.4551 - val_acc: 0.9090\n",
      "Epoch 68/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1490 - acc: 0.9423 - val_loss: 0.4554 - val_acc: 0.9083\n",
      "Epoch 69/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1471 - acc: 0.9387 - val_loss: 0.4558 - val_acc: 0.9083\n",
      "Epoch 70/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1444 - acc: 0.9444 - val_loss: 0.4544 - val_acc: 0.9086\n",
      "Epoch 71/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1395 - acc: 0.9454 - val_loss: 0.4545 - val_acc: 0.9090\n",
      "Epoch 72/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1319 - acc: 0.9493 - val_loss: 0.4503 - val_acc: 0.9083\n",
      "The 1th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 33ms/step - loss: 0.5325 - acc: 0.7521 - val_loss: 1.1303 - val_acc: 0.7859\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.4655 - acc: 0.7888 - val_loss: 0.9955 - val_acc: 0.8218\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.4214 - acc: 0.8075 - val_loss: 0.8789 - val_acc: 0.8455\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.3735 - acc: 0.8369 - val_loss: 0.7974 - val_acc: 0.8651\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 301us/step - loss: 0.3546 - acc: 0.8496 - val_loss: 0.7549 - val_acc: 0.8728\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.3229 - acc: 0.8654 - val_loss: 0.6885 - val_acc: 0.8724\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 295us/step - loss: 0.3060 - acc: 0.8783 - val_loss: 0.6294 - val_acc: 0.8760\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.2847 - acc: 0.8855 - val_loss: 0.5928 - val_acc: 0.8792\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 346us/step - loss: 0.2748 - acc: 0.8845 - val_loss: 0.5319 - val_acc: 0.8830\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 344us/step - loss: 0.2537 - acc: 0.8988 - val_loss: 0.4747 - val_acc: 0.8827\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.2634 - acc: 0.8899 - val_loss: 0.4682 - val_acc: 0.8853\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.2458 - acc: 0.9000 - val_loss: 0.5133 - val_acc: 0.8798\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.2433 - acc: 0.9092 - val_loss: 0.4310 - val_acc: 0.8994\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.2347 - acc: 0.9088 - val_loss: 0.4404 - val_acc: 0.8981\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.2241 - acc: 0.9154 - val_loss: 0.4739 - val_acc: 0.8926\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.2106 - acc: 0.9154 - val_loss: 0.4654 - val_acc: 0.8952\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1986 - acc: 0.9236 - val_loss: 0.4902 - val_acc: 0.8965\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.2034 - acc: 0.9203 - val_loss: 0.4718 - val_acc: 0.8962\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.2053 - acc: 0.9229 - val_loss: 0.4667 - val_acc: 0.8952\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1983 - acc: 0.9224 - val_loss: 0.4693 - val_acc: 0.9006\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1873 - acc: 0.9243 - val_loss: 0.4299 - val_acc: 0.9096\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.1821 - acc: 0.9276 - val_loss: 0.4272 - val_acc: 0.9109\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.1926 - acc: 0.9242 - val_loss: 0.4326 - val_acc: 0.9122\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1725 - acc: 0.9316 - val_loss: 0.4562 - val_acc: 0.9122\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1745 - acc: 0.9264 - val_loss: 0.4610 - val_acc: 0.9115\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1652 - acc: 0.9345 - val_loss: 0.4586 - val_acc: 0.9131\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.1600 - acc: 0.9379 - val_loss: 0.4319 - val_acc: 0.9173\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1657 - acc: 0.9364 - val_loss: 0.4357 - val_acc: 0.9167\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1631 - acc: 0.9356 - val_loss: 0.4357 - val_acc: 0.9167\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.1538 - acc: 0.9371 - val_loss: 0.4695 - val_acc: 0.9125\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.1571 - acc: 0.9393 - val_loss: 0.3858 - val_acc: 0.9199\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.1496 - acc: 0.9408 - val_loss: 0.3788 - val_acc: 0.9228\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1499 - acc: 0.9426 - val_loss: 0.3938 - val_acc: 0.9173\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1461 - acc: 0.9456 - val_loss: 0.4038 - val_acc: 0.9167\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1436 - acc: 0.9455 - val_loss: 0.4025 - val_acc: 0.9196\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1514 - acc: 0.9379 - val_loss: 0.3901 - val_acc: 0.9205\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1350 - acc: 0.9474 - val_loss: 0.3880 - val_acc: 0.9234\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1346 - acc: 0.9452 - val_loss: 0.3969 - val_acc: 0.9212\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1362 - acc: 0.9420 - val_loss: 0.4162 - val_acc: 0.9173\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.1360 - acc: 0.9476 - val_loss: 0.4279 - val_acc: 0.9173\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1433 - acc: 0.9448 - val_loss: 0.4191 - val_acc: 0.9215\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 295us/step - loss: 0.1347 - acc: 0.9454 - val_loss: 0.3905 - val_acc: 0.9237\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1316 - acc: 0.9487 - val_loss: 0.3948 - val_acc: 0.9237\n",
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1281 - acc: 0.9529 - val_loss: 0.4002 - val_acc: 0.9224\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1223 - acc: 0.9510 - val_loss: 0.4034 - val_acc: 0.9237\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1156 - acc: 0.9542 - val_loss: 0.5215 - val_acc: 0.9160\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1150 - acc: 0.9540 - val_loss: 0.5325 - val_acc: 0.9154\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1118 - acc: 0.9561 - val_loss: 0.5412 - val_acc: 0.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1191 - acc: 0.9550 - val_loss: 0.5573 - val_acc: 0.9157\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.1161 - acc: 0.9545 - val_loss: 0.5589 - val_acc: 0.9157\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.1075 - acc: 0.9563 - val_loss: 0.5571 - val_acc: 0.9154\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1133 - acc: 0.9538 - val_loss: 0.5575 - val_acc: 0.9154\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1155 - acc: 0.9550 - val_loss: 0.5575 - val_acc: 0.9154\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.1082 - acc: 0.9574 - val_loss: 0.5572 - val_acc: 0.9154\n",
      "Epoch 55/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1123 - acc: 0.9557 - val_loss: 0.5572 - val_acc: 0.9154\n",
      "Epoch 56/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1005 - acc: 0.9584 - val_loss: 0.5559 - val_acc: 0.9154\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1127 - acc: 0.9591 - val_loss: 0.5550 - val_acc: 0.9151\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.1155 - acc: 0.9537 - val_loss: 0.5543 - val_acc: 0.9151\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1025 - acc: 0.9592 - val_loss: 0.5548 - val_acc: 0.9151\n",
      "Epoch 60/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1077 - acc: 0.9584 - val_loss: 0.5534 - val_acc: 0.9154\n",
      "Epoch 61/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.1143 - acc: 0.9551 - val_loss: 0.5528 - val_acc: 0.9157\n",
      "Epoch 62/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1118 - acc: 0.9588 - val_loss: 0.5517 - val_acc: 0.9154\n",
      "The 2th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 33ms/step - loss: 0.4795 - acc: 0.7694 - val_loss: 0.6562 - val_acc: 0.8375\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 331us/step - loss: 0.4146 - acc: 0.8152 - val_loss: 0.5656 - val_acc: 0.8574\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 356us/step - loss: 0.4116 - acc: 0.8119 - val_loss: 0.4578 - val_acc: 0.8735\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 342us/step - loss: 0.3739 - acc: 0.8429 - val_loss: 0.4277 - val_acc: 0.8839\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 337us/step - loss: 0.3424 - acc: 0.8512 - val_loss: 0.4166 - val_acc: 0.8899\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.3410 - acc: 0.8587 - val_loss: 0.4024 - val_acc: 0.8997\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.3166 - acc: 0.8698 - val_loss: 0.3854 - val_acc: 0.9036\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.3007 - acc: 0.8768 - val_loss: 0.3623 - val_acc: 0.9051\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.2788 - acc: 0.8887 - val_loss: 0.3493 - val_acc: 0.9095\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.2766 - acc: 0.8912 - val_loss: 0.3452 - val_acc: 0.9134\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 334us/step - loss: 0.2584 - acc: 0.8973 - val_loss: 0.3421 - val_acc: 0.9131\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.2407 - acc: 0.9101 - val_loss: 0.3383 - val_acc: 0.9164\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.2497 - acc: 0.9075 - val_loss: 0.3336 - val_acc: 0.9173\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.2340 - acc: 0.9096 - val_loss: 0.3317 - val_acc: 0.9173\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 272us/step - loss: 0.2251 - acc: 0.9124 - val_loss: 0.3275 - val_acc: 0.9179\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 274us/step - loss: 0.2200 - acc: 0.9129 - val_loss: 0.3252 - val_acc: 0.9202\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 270us/step - loss: 0.2237 - acc: 0.9120 - val_loss: 0.3238 - val_acc: 0.9214\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 285us/step - loss: 0.2093 - acc: 0.9171 - val_loss: 0.3256 - val_acc: 0.9223\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.2048 - acc: 0.9219 - val_loss: 0.3255 - val_acc: 0.9232\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 265us/step - loss: 0.1916 - acc: 0.9247 - val_loss: 0.3263 - val_acc: 0.9250\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 270us/step - loss: 0.1988 - acc: 0.9232 - val_loss: 0.3287 - val_acc: 0.9277\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.1835 - acc: 0.9289 - val_loss: 0.3257 - val_acc: 0.9313\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 282us/step - loss: 0.1871 - acc: 0.9309 - val_loss: 0.3233 - val_acc: 0.9333\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.1857 - acc: 0.9316 - val_loss: 0.3152 - val_acc: 0.9351\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 282us/step - loss: 0.1757 - acc: 0.9325 - val_loss: 0.3135 - val_acc: 0.9363\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1741 - acc: 0.9371 - val_loss: 0.3136 - val_acc: 0.9366\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.1701 - acc: 0.9374 - val_loss: 0.3133 - val_acc: 0.9360\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 279us/step - loss: 0.1654 - acc: 0.9364 - val_loss: 0.3122 - val_acc: 0.9363\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 269us/step - loss: 0.1645 - acc: 0.9376 - val_loss: 0.3111 - val_acc: 0.9351\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 282us/step - loss: 0.1548 - acc: 0.9427 - val_loss: 0.3123 - val_acc: 0.9369\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 267us/step - loss: 0.1602 - acc: 0.9394 - val_loss: 0.3124 - val_acc: 0.9375\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.1518 - acc: 0.9434 - val_loss: 0.3107 - val_acc: 0.9384\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.1481 - acc: 0.9430 - val_loss: 0.3097 - val_acc: 0.9384\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1497 - acc: 0.9424 - val_loss: 0.3104 - val_acc: 0.9378\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 339us/step - loss: 0.1413 - acc: 0.9465 - val_loss: 0.3115 - val_acc: 0.9381\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1474 - acc: 0.9433 - val_loss: 0.3163 - val_acc: 0.9348\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1328 - acc: 0.9522 - val_loss: 0.3254 - val_acc: 0.9351\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1351 - acc: 0.9458 - val_loss: 0.3273 - val_acc: 0.9363\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1454 - acc: 0.9450 - val_loss: 0.3242 - val_acc: 0.9372\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1337 - acc: 0.9490 - val_loss: 0.3236 - val_acc: 0.9372\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 269us/step - loss: 0.1346 - acc: 0.9466 - val_loss: 0.3230 - val_acc: 0.9378\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 273us/step - loss: 0.1304 - acc: 0.9500 - val_loss: 0.3226 - val_acc: 0.9372\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1356 - acc: 0.9474 - val_loss: 0.3235 - val_acc: 0.9372\n",
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.1314 - acc: 0.9488 - val_loss: 0.3240 - val_acc: 0.9369\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.1350 - acc: 0.9495 - val_loss: 0.3244 - val_acc: 0.9366\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 279us/step - loss: 0.1290 - acc: 0.9512 - val_loss: 0.3231 - val_acc: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 268us/step - loss: 0.1257 - acc: 0.9523 - val_loss: 0.3231 - val_acc: 0.9369\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 271us/step - loss: 0.1311 - acc: 0.9485 - val_loss: 0.3230 - val_acc: 0.9369\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 270us/step - loss: 0.1197 - acc: 0.9527 - val_loss: 0.3228 - val_acc: 0.9372\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 274us/step - loss: 0.1268 - acc: 0.9523 - val_loss: 0.3225 - val_acc: 0.9366\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.1269 - acc: 0.9501 - val_loss: 0.3223 - val_acc: 0.9369\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.1320 - acc: 0.9514 - val_loss: 0.3226 - val_acc: 0.9369\n",
      "The 3th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 33ms/step - loss: 0.4546 - acc: 0.7819 - val_loss: 0.6589 - val_acc: 0.8417\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.4349 - acc: 0.7943 - val_loss: 0.5842 - val_acc: 0.8664\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.3915 - acc: 0.8228 - val_loss: 0.4488 - val_acc: 0.8809\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 281us/step - loss: 0.3723 - acc: 0.8392 - val_loss: 0.4242 - val_acc: 0.8963\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.3279 - acc: 0.8644 - val_loss: 0.4234 - val_acc: 0.9077\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 293us/step - loss: 0.3039 - acc: 0.8771 - val_loss: 0.4318 - val_acc: 0.9136\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.2989 - acc: 0.8838 - val_loss: 0.4298 - val_acc: 0.9182\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.2852 - acc: 0.8835 - val_loss: 0.4233 - val_acc: 0.9210\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.2576 - acc: 0.9006 - val_loss: 0.4144 - val_acc: 0.9222\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 285us/step - loss: 0.2391 - acc: 0.9079 - val_loss: 0.4117 - val_acc: 0.9198\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.2255 - acc: 0.9172 - val_loss: 0.4105 - val_acc: 0.9191\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.2189 - acc: 0.9200 - val_loss: 0.4092 - val_acc: 0.9207\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.2146 - acc: 0.9219 - val_loss: 0.4037 - val_acc: 0.9235\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.2017 - acc: 0.9283 - val_loss: 0.3968 - val_acc: 0.9222\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1879 - acc: 0.9337 - val_loss: 0.4026 - val_acc: 0.9216\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.1858 - acc: 0.9349 - val_loss: 0.3965 - val_acc: 0.9231\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.1792 - acc: 0.9366 - val_loss: 0.3858 - val_acc: 0.9235\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.1841 - acc: 0.9337 - val_loss: 0.3927 - val_acc: 0.9256\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1640 - acc: 0.9396 - val_loss: 0.3882 - val_acc: 0.9278\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.1739 - acc: 0.9394 - val_loss: 0.3897 - val_acc: 0.9284\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1601 - acc: 0.9462 - val_loss: 0.3880 - val_acc: 0.9287\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1717 - acc: 0.9373 - val_loss: 0.3856 - val_acc: 0.9287\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.1586 - acc: 0.9396 - val_loss: 0.3860 - val_acc: 0.9293\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 293us/step - loss: 0.1476 - acc: 0.9459 - val_loss: 0.3863 - val_acc: 0.9309\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1376 - acc: 0.9499 - val_loss: 0.4004 - val_acc: 0.9287\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1444 - acc: 0.9438 - val_loss: 0.4145 - val_acc: 0.9293\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1450 - acc: 0.9465 - val_loss: 0.3974 - val_acc: 0.9306\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1431 - acc: 0.9459 - val_loss: 0.3886 - val_acc: 0.9346\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.1370 - acc: 0.9510 - val_loss: 0.3929 - val_acc: 0.9346\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.1311 - acc: 0.9511 - val_loss: 0.3848 - val_acc: 0.9336\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1446 - acc: 0.9477 - val_loss: 0.3917 - val_acc: 0.9321\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 276us/step - loss: 0.1268 - acc: 0.9539 - val_loss: 0.3860 - val_acc: 0.9302\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.1257 - acc: 0.9524 - val_loss: 0.3822 - val_acc: 0.9318\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 285us/step - loss: 0.1224 - acc: 0.9540 - val_loss: 0.3813 - val_acc: 0.9346\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1230 - acc: 0.9564 - val_loss: 0.3782 - val_acc: 0.9358\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.1275 - acc: 0.9498 - val_loss: 0.3764 - val_acc: 0.9367\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.1299 - acc: 0.9543 - val_loss: 0.3795 - val_acc: 0.9333\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 285us/step - loss: 0.1199 - acc: 0.9537 - val_loss: 0.3736 - val_acc: 0.9370\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.1123 - acc: 0.9565 - val_loss: 0.3724 - val_acc: 0.9380\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.1106 - acc: 0.9597 - val_loss: 0.3713 - val_acc: 0.9370\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.1082 - acc: 0.9571 - val_loss: 0.3552 - val_acc: 0.9367\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 282us/step - loss: 0.1073 - acc: 0.9599 - val_loss: 0.3509 - val_acc: 0.9370\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 293us/step - loss: 0.1086 - acc: 0.9604 - val_loss: 0.3546 - val_acc: 0.9367\n",
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.1121 - acc: 0.9553 - val_loss: 0.3546 - val_acc: 0.9364\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1049 - acc: 0.9621 - val_loss: 0.3564 - val_acc: 0.9361\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1047 - acc: 0.9630 - val_loss: 0.3552 - val_acc: 0.9349\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1049 - acc: 0.9601 - val_loss: 0.3545 - val_acc: 0.9355\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.0995 - acc: 0.9633 - val_loss: 0.3524 - val_acc: 0.9355\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1030 - acc: 0.9592 - val_loss: 0.3521 - val_acc: 0.9358\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 295us/step - loss: 0.1013 - acc: 0.9598 - val_loss: 0.3516 - val_acc: 0.9340\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.0944 - acc: 0.9633 - val_loss: 0.3515 - val_acc: 0.9352\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1061 - acc: 0.9591 - val_loss: 0.3512 - val_acc: 0.9355\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.0971 - acc: 0.9641 - val_loss: 0.3504 - val_acc: 0.9358\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 0s 285us/step - loss: 0.1005 - acc: 0.9636 - val_loss: 0.3503 - val_acc: 0.9355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.0976 - acc: 0.9642 - val_loss: 0.3503 - val_acc: 0.9358\n",
      "Epoch 56/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.0963 - acc: 0.9615 - val_loss: 0.3496 - val_acc: 0.9358\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 0s 281us/step - loss: 0.1120 - acc: 0.9589 - val_loss: 0.3498 - val_acc: 0.9352\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1044 - acc: 0.9609 - val_loss: 0.3502 - val_acc: 0.9349\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.0935 - acc: 0.9641 - val_loss: 0.3494 - val_acc: 0.9346\n",
      "The 4th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 33ms/step - loss: 0.4837 - acc: 0.7739 - val_loss: 1.0736 - val_acc: 0.8112\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.4519 - acc: 0.7943 - val_loss: 1.0052 - val_acc: 0.8356\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.4005 - acc: 0.8170 - val_loss: 0.8054 - val_acc: 0.8551\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.3803 - acc: 0.8376 - val_loss: 0.6734 - val_acc: 0.8692\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.3711 - acc: 0.8450 - val_loss: 0.6591 - val_acc: 0.8715\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 301us/step - loss: 0.3342 - acc: 0.8620 - val_loss: 0.6282 - val_acc: 0.8747\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.3295 - acc: 0.8578 - val_loss: 0.6261 - val_acc: 0.8821\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.3211 - acc: 0.8676 - val_loss: 0.4805 - val_acc: 0.8955\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.3007 - acc: 0.8778 - val_loss: 0.4787 - val_acc: 0.8984\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 317us/step - loss: 0.2774 - acc: 0.8902 - val_loss: 0.4872 - val_acc: 0.9013\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.2702 - acc: 0.8891 - val_loss: 0.4850 - val_acc: 0.9019\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.2582 - acc: 0.8952 - val_loss: 0.4602 - val_acc: 0.9022\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.2423 - acc: 0.9003 - val_loss: 0.4169 - val_acc: 0.9071\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.2440 - acc: 0.9068 - val_loss: 0.4628 - val_acc: 0.9029\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.2331 - acc: 0.9080 - val_loss: 0.4427 - val_acc: 0.9032\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 301us/step - loss: 0.2238 - acc: 0.9094 - val_loss: 0.4784 - val_acc: 0.9054\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.2177 - acc: 0.9143 - val_loss: 0.4946 - val_acc: 0.9061\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.2142 - acc: 0.9180 - val_loss: 0.4970 - val_acc: 0.9064\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 301us/step - loss: 0.2068 - acc: 0.9175 - val_loss: 0.4964 - val_acc: 0.9080\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1963 - acc: 0.9237 - val_loss: 0.5089 - val_acc: 0.9083\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 312us/step - loss: 0.1997 - acc: 0.9196 - val_loss: 0.4951 - val_acc: 0.9109\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1906 - acc: 0.9280 - val_loss: 0.4446 - val_acc: 0.9083\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.1843 - acc: 0.9270 - val_loss: 0.4831 - val_acc: 0.9103\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1709 - acc: 0.9337 - val_loss: 0.4649 - val_acc: 0.9119\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1751 - acc: 0.9294 - val_loss: 0.4754 - val_acc: 0.9131\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1635 - acc: 0.9365 - val_loss: 0.4719 - val_acc: 0.9144\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1678 - acc: 0.9304 - val_loss: 0.4704 - val_acc: 0.9144\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1600 - acc: 0.9373 - val_loss: 0.4587 - val_acc: 0.9160\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1574 - acc: 0.9377 - val_loss: 0.4305 - val_acc: 0.9154\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1499 - acc: 0.9408 - val_loss: 0.4251 - val_acc: 0.9154\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1546 - acc: 0.9414 - val_loss: 0.4462 - val_acc: 0.9167\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1545 - acc: 0.9412 - val_loss: 0.4442 - val_acc: 0.9163\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1539 - acc: 0.9394 - val_loss: 0.4153 - val_acc: 0.9186\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1361 - acc: 0.9455 - val_loss: 0.3967 - val_acc: 0.9192\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1408 - acc: 0.9469 - val_loss: 0.3806 - val_acc: 0.9167\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1328 - acc: 0.9468 - val_loss: 0.4647 - val_acc: 0.9183\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1410 - acc: 0.9455 - val_loss: 0.4360 - val_acc: 0.9196\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1375 - acc: 0.9450 - val_loss: 0.4590 - val_acc: 0.9196\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1329 - acc: 0.9465 - val_loss: 0.4718 - val_acc: 0.9199\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1313 - acc: 0.9504 - val_loss: 0.4604 - val_acc: 0.9234\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1266 - acc: 0.9521 - val_loss: 0.4693 - val_acc: 0.9208\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.1290 - acc: 0.9499 - val_loss: 0.4486 - val_acc: 0.9231\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1175 - acc: 0.9511 - val_loss: 0.4572 - val_acc: 0.9215\n",
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1243 - acc: 0.9499 - val_loss: 0.4580 - val_acc: 0.9199\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1173 - acc: 0.9538 - val_loss: 0.4709 - val_acc: 0.9218\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1200 - acc: 0.9519 - val_loss: 0.4672 - val_acc: 0.9221\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 310us/step - loss: 0.1213 - acc: 0.9521 - val_loss: 0.4626 - val_acc: 0.9221\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1138 - acc: 0.9565 - val_loss: 0.4660 - val_acc: 0.9224\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 317us/step - loss: 0.1211 - acc: 0.9535 - val_loss: 0.4588 - val_acc: 0.9228\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.1150 - acc: 0.9548 - val_loss: 0.4672 - val_acc: 0.9218\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.1234 - acc: 0.9507 - val_loss: 0.4652 - val_acc: 0.9231\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.1140 - acc: 0.9538 - val_loss: 0.4685 - val_acc: 0.9240\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.1208 - acc: 0.9499 - val_loss: 0.4689 - val_acc: 0.9231\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 0s 275us/step - loss: 0.1149 - acc: 0.9496 - val_loss: 0.4722 - val_acc: 0.9231\n",
      "Epoch 55/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1158 - acc: 0.9572 - val_loss: 0.4736 - val_acc: 0.9228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.1169 - acc: 0.9530 - val_loss: 0.4746 - val_acc: 0.9224\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.1150 - acc: 0.9546 - val_loss: 0.4727 - val_acc: 0.9224\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1145 - acc: 0.9556 - val_loss: 0.4748 - val_acc: 0.9221\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.1176 - acc: 0.9520 - val_loss: 0.4746 - val_acc: 0.9237\n",
      "Epoch 60/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1197 - acc: 0.9524 - val_loss: 0.4777 - val_acc: 0.9228\n",
      "Epoch 61/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1107 - acc: 0.9575 - val_loss: 0.4743 - val_acc: 0.9228\n",
      "Epoch 62/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.1069 - acc: 0.9589 - val_loss: 0.4763 - val_acc: 0.9228\n",
      "Epoch 63/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.1144 - acc: 0.9521 - val_loss: 0.4785 - val_acc: 0.9228\n",
      "Epoch 64/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.1102 - acc: 0.9558 - val_loss: 0.4770 - val_acc: 0.9234\n",
      "Epoch 65/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.1105 - acc: 0.9567 - val_loss: 0.4752 - val_acc: 0.9234\n",
      "Epoch 66/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1148 - acc: 0.9524 - val_loss: 0.4745 - val_acc: 0.9237\n",
      "Epoch 67/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.1144 - acc: 0.9542 - val_loss: 0.4772 - val_acc: 0.9237\n",
      "Epoch 68/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.1184 - acc: 0.9551 - val_loss: 0.4733 - val_acc: 0.9237\n",
      "Epoch 69/500\n",
      "484/484 [==============================] - 0s 285us/step - loss: 0.1173 - acc: 0.9522 - val_loss: 0.4743 - val_acc: 0.9231\n",
      "Epoch 70/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.1125 - acc: 0.9546 - val_loss: 0.4732 - val_acc: 0.9234\n",
      "Epoch 71/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.1209 - acc: 0.9515 - val_loss: 0.4721 - val_acc: 0.9237\n",
      "Epoch 72/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1181 - acc: 0.9522 - val_loss: 0.4757 - val_acc: 0.9231\n",
      "The 5th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 33ms/step - loss: 0.4563 - acc: 0.7895 - val_loss: 0.9662 - val_acc: 0.8320\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.4123 - acc: 0.8204 - val_loss: 0.8684 - val_acc: 0.8557\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.3815 - acc: 0.8351 - val_loss: 0.7400 - val_acc: 0.8720\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.3546 - acc: 0.8475 - val_loss: 0.5800 - val_acc: 0.8920\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.3209 - acc: 0.8622 - val_loss: 0.5763 - val_acc: 0.8987\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.3114 - acc: 0.8698 - val_loss: 0.5328 - val_acc: 0.9033\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.2939 - acc: 0.8783 - val_loss: 0.5563 - val_acc: 0.9057\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.2787 - acc: 0.8892 - val_loss: 0.5098 - val_acc: 0.9107\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.2657 - acc: 0.8969 - val_loss: 0.4112 - val_acc: 0.9170\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.2501 - acc: 0.9034 - val_loss: 0.3957 - val_acc: 0.9160\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.2365 - acc: 0.9097 - val_loss: 0.3975 - val_acc: 0.9200\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.2295 - acc: 0.9121 - val_loss: 0.4082 - val_acc: 0.9200\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.2076 - acc: 0.9210 - val_loss: 0.4078 - val_acc: 0.9220\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.2132 - acc: 0.9193 - val_loss: 0.3935 - val_acc: 0.9243\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.2023 - acc: 0.9268 - val_loss: 0.3883 - val_acc: 0.9243\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 293us/step - loss: 0.1994 - acc: 0.9261 - val_loss: 0.3794 - val_acc: 0.9267\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.1823 - acc: 0.9302 - val_loss: 0.3870 - val_acc: 0.9283\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.1717 - acc: 0.9331 - val_loss: 0.3855 - val_acc: 0.9293\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.1671 - acc: 0.9379 - val_loss: 0.3747 - val_acc: 0.9260\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 282us/step - loss: 0.1764 - acc: 0.9350 - val_loss: 0.3690 - val_acc: 0.9283\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.1588 - acc: 0.9378 - val_loss: 0.3494 - val_acc: 0.9287\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.1702 - acc: 0.9388 - val_loss: 0.3573 - val_acc: 0.9300\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1597 - acc: 0.9397 - val_loss: 0.3584 - val_acc: 0.9290\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.1514 - acc: 0.9411 - val_loss: 0.3468 - val_acc: 0.9313\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 303us/step - loss: 0.1419 - acc: 0.9464 - val_loss: 0.3458 - val_acc: 0.9327\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1400 - acc: 0.9478 - val_loss: 0.3502 - val_acc: 0.9320\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1542 - acc: 0.9442 - val_loss: 0.3459 - val_acc: 0.9340\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1379 - acc: 0.9493 - val_loss: 0.3420 - val_acc: 0.9340\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.1387 - acc: 0.9479 - val_loss: 0.3416 - val_acc: 0.9353\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 302us/step - loss: 0.1365 - acc: 0.9483 - val_loss: 0.3458 - val_acc: 0.9367\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1280 - acc: 0.9512 - val_loss: 0.3209 - val_acc: 0.9367\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.1268 - acc: 0.9521 - val_loss: 0.3150 - val_acc: 0.9400\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1274 - acc: 0.9526 - val_loss: 0.3164 - val_acc: 0.9427\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1190 - acc: 0.9564 - val_loss: 0.3181 - val_acc: 0.9443\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 292us/step - loss: 0.1150 - acc: 0.9552 - val_loss: 0.3157 - val_acc: 0.9400\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.1212 - acc: 0.9551 - val_loss: 0.3124 - val_acc: 0.9393\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 295us/step - loss: 0.1164 - acc: 0.9552 - val_loss: 0.3099 - val_acc: 0.9413\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 291us/step - loss: 0.1116 - acc: 0.9580 - val_loss: 0.3021 - val_acc: 0.9407\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1096 - acc: 0.9556 - val_loss: 0.3061 - val_acc: 0.9440\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.1104 - acc: 0.9591 - val_loss: 0.2997 - val_acc: 0.9460\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 279us/step - loss: 0.1104 - acc: 0.9564 - val_loss: 0.3070 - val_acc: 0.9393\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 271us/step - loss: 0.1126 - acc: 0.9569 - val_loss: 0.3172 - val_acc: 0.9407\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 276us/step - loss: 0.0999 - acc: 0.9644 - val_loss: 0.3259 - val_acc: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.0995 - acc: 0.9604 - val_loss: 0.3295 - val_acc: 0.9360\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 281us/step - loss: 0.0992 - acc: 0.9601 - val_loss: 0.3520 - val_acc: 0.9357\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.1036 - acc: 0.9599 - val_loss: 0.3527 - val_acc: 0.9350\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.0991 - acc: 0.9638 - val_loss: 0.3246 - val_acc: 0.9363\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 290us/step - loss: 0.0960 - acc: 0.9631 - val_loss: 0.3241 - val_acc: 0.9370\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 279us/step - loss: 0.1079 - acc: 0.9567 - val_loss: 0.3242 - val_acc: 0.9360\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.1045 - acc: 0.9585 - val_loss: 0.3214 - val_acc: 0.9377\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.1100 - acc: 0.9567 - val_loss: 0.3217 - val_acc: 0.9383\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.0992 - acc: 0.9626 - val_loss: 0.3225 - val_acc: 0.9393\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 0s 279us/step - loss: 0.0975 - acc: 0.9627 - val_loss: 0.3220 - val_acc: 0.9400\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.0950 - acc: 0.9648 - val_loss: 0.3205 - val_acc: 0.9397\n",
      "Epoch 55/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.0952 - acc: 0.9639 - val_loss: 0.3201 - val_acc: 0.9393\n",
      "Epoch 56/500\n",
      "484/484 [==============================] - 0s 282us/step - loss: 0.0946 - acc: 0.9632 - val_loss: 0.3208 - val_acc: 0.9393\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.0998 - acc: 0.9598 - val_loss: 0.3201 - val_acc: 0.9393\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 0s 277us/step - loss: 0.1082 - acc: 0.9588 - val_loss: 0.3212 - val_acc: 0.9393\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.0986 - acc: 0.9620 - val_loss: 0.3203 - val_acc: 0.9393\n",
      "Epoch 60/500\n",
      "484/484 [==============================] - 0s 276us/step - loss: 0.0945 - acc: 0.9655 - val_loss: 0.3216 - val_acc: 0.9393\n",
      "The 6th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 33ms/step - loss: 0.5131 - acc: 0.7635 - val_loss: 0.7568 - val_acc: 0.8179\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 301us/step - loss: 0.4497 - acc: 0.7904 - val_loss: 0.7412 - val_acc: 0.8448\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.4013 - acc: 0.8197 - val_loss: 0.6493 - val_acc: 0.8679\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.3661 - acc: 0.8471 - val_loss: 0.6214 - val_acc: 0.8827\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.3347 - acc: 0.8613 - val_loss: 0.4981 - val_acc: 0.8969\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.3258 - acc: 0.8626 - val_loss: 0.5237 - val_acc: 0.9006\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.2980 - acc: 0.8737 - val_loss: 0.4446 - val_acc: 0.9015\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.2723 - acc: 0.8881 - val_loss: 0.4100 - val_acc: 0.9059\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 298us/step - loss: 0.2722 - acc: 0.8946 - val_loss: 0.4124 - val_acc: 0.9068\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.2482 - acc: 0.9014 - val_loss: 0.4133 - val_acc: 0.9096\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.2488 - acc: 0.8998 - val_loss: 0.4122 - val_acc: 0.9114\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.2420 - acc: 0.9078 - val_loss: 0.3865 - val_acc: 0.9136\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.2272 - acc: 0.9141 - val_loss: 0.3844 - val_acc: 0.9127\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 304us/step - loss: 0.2236 - acc: 0.9171 - val_loss: 0.3893 - val_acc: 0.9136\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.2115 - acc: 0.9161 - val_loss: 0.3872 - val_acc: 0.9130\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.2104 - acc: 0.9150 - val_loss: 0.3881 - val_acc: 0.9151\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 312us/step - loss: 0.1971 - acc: 0.9282 - val_loss: 0.3914 - val_acc: 0.9170\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.2021 - acc: 0.9223 - val_loss: 0.3845 - val_acc: 0.9179\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.2045 - acc: 0.9221 - val_loss: 0.3813 - val_acc: 0.9157\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 317us/step - loss: 0.1919 - acc: 0.9252 - val_loss: 0.3804 - val_acc: 0.9154\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1792 - acc: 0.9320 - val_loss: 0.3784 - val_acc: 0.9185\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 308us/step - loss: 0.1848 - acc: 0.9251 - val_loss: 0.3816 - val_acc: 0.9170\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.1875 - acc: 0.9265 - val_loss: 0.3913 - val_acc: 0.9170\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.1777 - acc: 0.9281 - val_loss: 0.3930 - val_acc: 0.9176\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1734 - acc: 0.9329 - val_loss: 0.3944 - val_acc: 0.9164\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1727 - acc: 0.9333 - val_loss: 0.3967 - val_acc: 0.9151\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.1634 - acc: 0.9352 - val_loss: 0.4042 - val_acc: 0.9157\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.1777 - acc: 0.9311 - val_loss: 0.4066 - val_acc: 0.9176\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 335us/step - loss: 0.1583 - acc: 0.9372 - val_loss: 0.4075 - val_acc: 0.9176\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.1523 - acc: 0.9436 - val_loss: 0.4055 - val_acc: 0.9182\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1575 - acc: 0.9379 - val_loss: 0.4052 - val_acc: 0.9176\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1627 - acc: 0.9379 - val_loss: 0.4043 - val_acc: 0.9176\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1544 - acc: 0.9364 - val_loss: 0.4024 - val_acc: 0.9179\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1505 - acc: 0.9403 - val_loss: 0.4014 - val_acc: 0.9185\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1576 - acc: 0.9392 - val_loss: 0.4021 - val_acc: 0.9191\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 311us/step - loss: 0.1593 - acc: 0.9366 - val_loss: 0.4006 - val_acc: 0.9194\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.1566 - acc: 0.9371 - val_loss: 0.3981 - val_acc: 0.9198\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1482 - acc: 0.9389 - val_loss: 0.3946 - val_acc: 0.9198\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.1736 - acc: 0.9315 - val_loss: 0.3922 - val_acc: 0.9194\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1581 - acc: 0.9360 - val_loss: 0.3912 - val_acc: 0.9201\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1500 - acc: 0.9403 - val_loss: 0.3879 - val_acc: 0.9204\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.1557 - acc: 0.9382 - val_loss: 0.3867 - val_acc: 0.9207\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 313us/step - loss: 0.1458 - acc: 0.9428 - val_loss: 0.3869 - val_acc: 0.9213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1480 - acc: 0.9425 - val_loss: 0.3896 - val_acc: 0.9216\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 312us/step - loss: 0.1508 - acc: 0.9412 - val_loss: 0.3900 - val_acc: 0.9219\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1409 - acc: 0.9455 - val_loss: 0.3905 - val_acc: 0.9219\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1372 - acc: 0.9457 - val_loss: 0.3911 - val_acc: 0.9216\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1457 - acc: 0.9418 - val_loss: 0.3906 - val_acc: 0.9216\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.1551 - acc: 0.9415 - val_loss: 0.3904 - val_acc: 0.9213\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1405 - acc: 0.9436 - val_loss: 0.3893 - val_acc: 0.9213\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1349 - acc: 0.9477 - val_loss: 0.3898 - val_acc: 0.9210\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1351 - acc: 0.9471 - val_loss: 0.3905 - val_acc: 0.9207\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.1503 - acc: 0.9415 - val_loss: 0.3898 - val_acc: 0.9207\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1451 - acc: 0.9439 - val_loss: 0.3894 - val_acc: 0.9210\n",
      "Epoch 55/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.1470 - acc: 0.9405 - val_loss: 0.3888 - val_acc: 0.9210\n",
      "Epoch 56/500\n",
      "484/484 [==============================] - 0s 317us/step - loss: 0.1449 - acc: 0.9435 - val_loss: 0.3899 - val_acc: 0.9210\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.1360 - acc: 0.9468 - val_loss: 0.3894 - val_acc: 0.9213\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1484 - acc: 0.9425 - val_loss: 0.3896 - val_acc: 0.9207\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1425 - acc: 0.9444 - val_loss: 0.3895 - val_acc: 0.9213\n",
      "Epoch 60/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1355 - acc: 0.9472 - val_loss: 0.3889 - val_acc: 0.9210\n",
      "Epoch 61/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1419 - acc: 0.9435 - val_loss: 0.3892 - val_acc: 0.9213\n",
      "Epoch 62/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1364 - acc: 0.9471 - val_loss: 0.3892 - val_acc: 0.9216\n",
      "Epoch 63/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1350 - acc: 0.9504 - val_loss: 0.3884 - val_acc: 0.9219\n",
      "Epoch 64/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1478 - acc: 0.9411 - val_loss: 0.3885 - val_acc: 0.9213\n",
      "Epoch 65/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1380 - acc: 0.9458 - val_loss: 0.3889 - val_acc: 0.9213\n",
      "The 7th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 34ms/step - loss: 0.4809 - acc: 0.7652 - val_loss: 0.7320 - val_acc: 0.8388\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.4311 - acc: 0.7935 - val_loss: 0.7427 - val_acc: 0.8547\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.3830 - acc: 0.8211 - val_loss: 0.6237 - val_acc: 0.8692\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.3596 - acc: 0.8374 - val_loss: 0.5908 - val_acc: 0.8757\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.3275 - acc: 0.8612 - val_loss: 0.5801 - val_acc: 0.8848\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.3239 - acc: 0.8650 - val_loss: 0.5622 - val_acc: 0.8913\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.2911 - acc: 0.8751 - val_loss: 0.5781 - val_acc: 0.8957\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 312us/step - loss: 0.2714 - acc: 0.8925 - val_loss: 0.5468 - val_acc: 0.8942\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.2761 - acc: 0.8865 - val_loss: 0.5342 - val_acc: 0.8953\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.2547 - acc: 0.9002 - val_loss: 0.5236 - val_acc: 0.8982\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 294us/step - loss: 0.2418 - acc: 0.9085 - val_loss: 0.5207 - val_acc: 0.9011\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 295us/step - loss: 0.2401 - acc: 0.9056 - val_loss: 0.5225 - val_acc: 0.9011\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 295us/step - loss: 0.2273 - acc: 0.9084 - val_loss: 0.5206 - val_acc: 0.9018\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 291us/step - loss: 0.2232 - acc: 0.9131 - val_loss: 0.5170 - val_acc: 0.9054\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 297us/step - loss: 0.2250 - acc: 0.9104 - val_loss: 0.5162 - val_acc: 0.9080\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.2158 - acc: 0.9156 - val_loss: 0.5175 - val_acc: 0.9087\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.2180 - acc: 0.9156 - val_loss: 0.4950 - val_acc: 0.9116\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.1999 - acc: 0.9256 - val_loss: 0.5013 - val_acc: 0.9116\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.1955 - acc: 0.9245 - val_loss: 0.5040 - val_acc: 0.9141\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1801 - acc: 0.9290 - val_loss: 0.5057 - val_acc: 0.9149\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 314us/step - loss: 0.1858 - acc: 0.9289 - val_loss: 0.5050 - val_acc: 0.9149\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 305us/step - loss: 0.1862 - acc: 0.9274 - val_loss: 0.4984 - val_acc: 0.9152\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 307us/step - loss: 0.1815 - acc: 0.9299 - val_loss: 0.4842 - val_acc: 0.9174\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 300us/step - loss: 0.1751 - acc: 0.9315 - val_loss: 0.4874 - val_acc: 0.9167\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1722 - acc: 0.9286 - val_loss: 0.4992 - val_acc: 0.9152\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 306us/step - loss: 0.1791 - acc: 0.9271 - val_loss: 0.5022 - val_acc: 0.9159\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1705 - acc: 0.9350 - val_loss: 0.5019 - val_acc: 0.9145\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.1544 - acc: 0.9386 - val_loss: 0.4909 - val_acc: 0.9156\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 296us/step - loss: 0.1618 - acc: 0.9378 - val_loss: 0.4858 - val_acc: 0.9159\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 299us/step - loss: 0.1700 - acc: 0.9332 - val_loss: 0.4885 - val_acc: 0.9141\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 301us/step - loss: 0.1574 - acc: 0.9353 - val_loss: 0.4883 - val_acc: 0.9145\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.1510 - acc: 0.9424 - val_loss: 0.4889 - val_acc: 0.9149\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 280us/step - loss: 0.1592 - acc: 0.9378 - val_loss: 0.4879 - val_acc: 0.9170\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 283us/step - loss: 0.1549 - acc: 0.9412 - val_loss: 0.4877 - val_acc: 0.9174\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1549 - acc: 0.9404 - val_loss: 0.4886 - val_acc: 0.9170\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 287us/step - loss: 0.1397 - acc: 0.9456 - val_loss: 0.4892 - val_acc: 0.9159\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 286us/step - loss: 0.1521 - acc: 0.9412 - val_loss: 0.4901 - val_acc: 0.9149\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 288us/step - loss: 0.1399 - acc: 0.9465 - val_loss: 0.4909 - val_acc: 0.9149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 276us/step - loss: 0.1481 - acc: 0.9426 - val_loss: 0.4888 - val_acc: 0.9149\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 281us/step - loss: 0.1437 - acc: 0.9426 - val_loss: 0.4889 - val_acc: 0.9145\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 289us/step - loss: 0.1503 - acc: 0.9438 - val_loss: 0.4890 - val_acc: 0.9145\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 278us/step - loss: 0.1552 - acc: 0.9380 - val_loss: 0.4904 - val_acc: 0.9141\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 284us/step - loss: 0.1501 - acc: 0.9431 - val_loss: 0.4909 - val_acc: 0.9141\n",
      "The 8th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 16s 34ms/step - loss: 0.4485 - acc: 0.7863 - val_loss: 0.9267 - val_acc: 0.8292\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 309us/step - loss: 0.4074 - acc: 0.8194 - val_loss: 0.8753 - val_acc: 0.8482\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 317us/step - loss: 0.3973 - acc: 0.8301 - val_loss: 0.8156 - val_acc: 0.8563\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.3579 - acc: 0.8460 - val_loss: 0.7229 - val_acc: 0.8676\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.3418 - acc: 0.8588 - val_loss: 0.6505 - val_acc: 0.8801\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.3289 - acc: 0.8669 - val_loss: 0.6082 - val_acc: 0.8881\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.2896 - acc: 0.8853 - val_loss: 0.5289 - val_acc: 0.8967\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.2808 - acc: 0.8915 - val_loss: 0.4951 - val_acc: 0.9012\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.2515 - acc: 0.9024 - val_loss: 0.4687 - val_acc: 0.9068\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 321us/step - loss: 0.2467 - acc: 0.9065 - val_loss: 0.4616 - val_acc: 0.9098\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.2381 - acc: 0.9098 - val_loss: 0.4600 - val_acc: 0.9101\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.2255 - acc: 0.9204 - val_loss: 0.4137 - val_acc: 0.9146\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.2232 - acc: 0.9159 - val_loss: 0.4114 - val_acc: 0.9158\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.2225 - acc: 0.9137 - val_loss: 0.3834 - val_acc: 0.9190\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.2043 - acc: 0.9212 - val_loss: 0.3793 - val_acc: 0.9208\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 315us/step - loss: 0.1888 - acc: 0.9277 - val_loss: 0.3935 - val_acc: 0.9199\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1907 - acc: 0.9266 - val_loss: 0.4044 - val_acc: 0.9223\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1904 - acc: 0.9286 - val_loss: 0.4071 - val_acc: 0.9241\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1706 - acc: 0.9379 - val_loss: 0.3833 - val_acc: 0.9262\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1742 - acc: 0.9364 - val_loss: 0.4102 - val_acc: 0.9253\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 317us/step - loss: 0.1830 - acc: 0.9351 - val_loss: 0.4032 - val_acc: 0.9289\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1752 - acc: 0.9341 - val_loss: 0.3675 - val_acc: 0.9298\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 316us/step - loss: 0.1653 - acc: 0.9357 - val_loss: 0.3795 - val_acc: 0.9268\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1686 - acc: 0.9326 - val_loss: 0.3282 - val_acc: 0.9283\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1582 - acc: 0.9427 - val_loss: 0.3734 - val_acc: 0.9280\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1466 - acc: 0.9418 - val_loss: 0.3905 - val_acc: 0.9274\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.1532 - acc: 0.9421 - val_loss: 0.3937 - val_acc: 0.9283\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1467 - acc: 0.9445 - val_loss: 0.3925 - val_acc: 0.9259\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1457 - acc: 0.9460 - val_loss: 0.3830 - val_acc: 0.9280\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 321us/step - loss: 0.1483 - acc: 0.9424 - val_loss: 0.3828 - val_acc: 0.9280\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1483 - acc: 0.9447 - val_loss: 0.3779 - val_acc: 0.9286\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1384 - acc: 0.9498 - val_loss: 0.3771 - val_acc: 0.9283\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1435 - acc: 0.9439 - val_loss: 0.3753 - val_acc: 0.9283\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1477 - acc: 0.9446 - val_loss: 0.3754 - val_acc: 0.9280\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.1406 - acc: 0.9461 - val_loss: 0.3749 - val_acc: 0.9280\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1336 - acc: 0.9512 - val_loss: 0.3744 - val_acc: 0.9277\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1412 - acc: 0.9466 - val_loss: 0.3739 - val_acc: 0.9280\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1445 - acc: 0.9442 - val_loss: 0.3871 - val_acc: 0.9271\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.1401 - acc: 0.9483 - val_loss: 0.3865 - val_acc: 0.9268\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1256 - acc: 0.9523 - val_loss: 0.3843 - val_acc: 0.9274\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1429 - acc: 0.9456 - val_loss: 0.3727 - val_acc: 0.9280\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.1345 - acc: 0.9487 - val_loss: 0.3730 - val_acc: 0.9280\n",
      "The 9th iteration is run\n",
      "Train on 484 samples, validate on 120 samples\n",
      "Epoch 1/500\n",
      "484/484 [==============================] - 17s 35ms/step - loss: 0.4797 - acc: 0.7713 - val_loss: 0.7552 - val_acc: 0.8296\n",
      "Epoch 2/500\n",
      "484/484 [==============================] - 0s 321us/step - loss: 0.4529 - acc: 0.7903 - val_loss: 0.6957 - val_acc: 0.8590\n",
      "Epoch 3/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.4105 - acc: 0.8148 - val_loss: 0.5865 - val_acc: 0.8753\n",
      "Epoch 4/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.3918 - acc: 0.8218 - val_loss: 0.6101 - val_acc: 0.8769\n",
      "Epoch 5/500\n",
      "484/484 [==============================] - 0s 330us/step - loss: 0.3598 - acc: 0.8424 - val_loss: 0.6037 - val_acc: 0.8775\n",
      "Epoch 6/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.3527 - acc: 0.8468 - val_loss: 0.5985 - val_acc: 0.8704\n",
      "Epoch 7/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.3276 - acc: 0.8636 - val_loss: 0.5968 - val_acc: 0.8682\n",
      "Epoch 8/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.3137 - acc: 0.8623 - val_loss: 0.6131 - val_acc: 0.8698\n",
      "Epoch 9/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.3136 - acc: 0.8660 - val_loss: 0.5773 - val_acc: 0.8759\n",
      "Epoch 10/500\n",
      "484/484 [==============================] - 0s 330us/step - loss: 0.2886 - acc: 0.8757 - val_loss: 0.5358 - val_acc: 0.8815\n",
      "Epoch 11/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.2965 - acc: 0.8770 - val_loss: 0.5250 - val_acc: 0.8836\n",
      "Epoch 12/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.2791 - acc: 0.8818 - val_loss: 0.5130 - val_acc: 0.8846\n",
      "Epoch 13/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.2641 - acc: 0.8867 - val_loss: 0.4988 - val_acc: 0.8910\n",
      "Epoch 14/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.2580 - acc: 0.8932 - val_loss: 0.5168 - val_acc: 0.8938\n",
      "Epoch 15/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.2590 - acc: 0.8904 - val_loss: 0.5276 - val_acc: 0.8957\n",
      "Epoch 16/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.2469 - acc: 0.9008 - val_loss: 0.4944 - val_acc: 0.8963\n",
      "Epoch 17/500\n",
      "484/484 [==============================] - 0s 312us/step - loss: 0.2362 - acc: 0.9001 - val_loss: 0.4932 - val_acc: 0.8978\n",
      "Epoch 18/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.2352 - acc: 0.9012 - val_loss: 0.4917 - val_acc: 0.8991\n",
      "Epoch 19/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.2426 - acc: 0.8926 - val_loss: 0.4913 - val_acc: 0.8997\n",
      "Epoch 20/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.2300 - acc: 0.9053 - val_loss: 0.5074 - val_acc: 0.8997\n",
      "Epoch 21/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.2164 - acc: 0.9142 - val_loss: 0.5220 - val_acc: 0.8975\n",
      "Epoch 22/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.2174 - acc: 0.9123 - val_loss: 0.5208 - val_acc: 0.8957\n",
      "Epoch 23/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.2112 - acc: 0.9135 - val_loss: 0.5106 - val_acc: 0.9003\n",
      "Epoch 24/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.2112 - acc: 0.9138 - val_loss: 0.4869 - val_acc: 0.9025\n",
      "Epoch 25/500\n",
      "484/484 [==============================] - 0s 321us/step - loss: 0.2019 - acc: 0.9140 - val_loss: 0.4981 - val_acc: 0.9019\n",
      "Epoch 26/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1928 - acc: 0.9194 - val_loss: 0.4969 - val_acc: 0.9019\n",
      "Epoch 27/500\n",
      "484/484 [==============================] - 0s 318us/step - loss: 0.1888 - acc: 0.9226 - val_loss: 0.4977 - val_acc: 0.9019\n",
      "Epoch 28/500\n",
      "484/484 [==============================] - 0s 317us/step - loss: 0.1941 - acc: 0.9206 - val_loss: 0.5025 - val_acc: 0.9059\n",
      "Epoch 29/500\n",
      "484/484 [==============================] - 0s 330us/step - loss: 0.1906 - acc: 0.9245 - val_loss: 0.4957 - val_acc: 0.9068\n",
      "Epoch 30/500\n",
      "484/484 [==============================] - 0s 334us/step - loss: 0.1854 - acc: 0.9220 - val_loss: 0.5020 - val_acc: 0.9074\n",
      "Epoch 31/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1756 - acc: 0.9285 - val_loss: 0.4790 - val_acc: 0.9080\n",
      "Epoch 32/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.1775 - acc: 0.9266 - val_loss: 0.4998 - val_acc: 0.9062\n",
      "Epoch 33/500\n",
      "484/484 [==============================] - 0s 321us/step - loss: 0.1779 - acc: 0.9267 - val_loss: 0.4920 - val_acc: 0.9052\n",
      "Epoch 34/500\n",
      "484/484 [==============================] - 0s 324us/step - loss: 0.1698 - acc: 0.9330 - val_loss: 0.5007 - val_acc: 0.9043\n",
      "Epoch 35/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.1660 - acc: 0.9342 - val_loss: 0.4540 - val_acc: 0.9043\n",
      "Epoch 36/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1631 - acc: 0.9350 - val_loss: 0.4457 - val_acc: 0.9037\n",
      "Epoch 37/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1558 - acc: 0.9364 - val_loss: 0.4343 - val_acc: 0.9083\n",
      "Epoch 38/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1651 - acc: 0.9372 - val_loss: 0.4318 - val_acc: 0.9056\n",
      "Epoch 39/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1530 - acc: 0.9397 - val_loss: 0.4332 - val_acc: 0.9031\n",
      "Epoch 40/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1531 - acc: 0.9365 - val_loss: 0.4443 - val_acc: 0.9034\n",
      "Epoch 41/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.1586 - acc: 0.9366 - val_loss: 0.4302 - val_acc: 0.9043\n",
      "Epoch 42/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1454 - acc: 0.9397 - val_loss: 0.4387 - val_acc: 0.9052\n",
      "Epoch 43/500\n",
      "484/484 [==============================] - 0s 320us/step - loss: 0.1403 - acc: 0.9448 - val_loss: 0.4369 - val_acc: 0.9062\n",
      "Epoch 44/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1478 - acc: 0.9413 - val_loss: 0.4456 - val_acc: 0.9049\n",
      "Epoch 45/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1376 - acc: 0.9454 - val_loss: 0.4428 - val_acc: 0.9052\n",
      "Epoch 46/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1514 - acc: 0.9399 - val_loss: 0.4265 - val_acc: 0.9083\n",
      "Epoch 47/500\n",
      "484/484 [==============================] - 0s 325us/step - loss: 0.1533 - acc: 0.9376 - val_loss: 0.4254 - val_acc: 0.9080\n",
      "Epoch 48/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1436 - acc: 0.9460 - val_loss: 0.4252 - val_acc: 0.9071\n",
      "Epoch 49/500\n",
      "484/484 [==============================] - 0s 332us/step - loss: 0.1452 - acc: 0.9428 - val_loss: 0.4264 - val_acc: 0.9080\n",
      "Epoch 50/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1552 - acc: 0.9380 - val_loss: 0.4271 - val_acc: 0.9077\n",
      "Epoch 51/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.1421 - acc: 0.9389 - val_loss: 0.4264 - val_acc: 0.9083\n",
      "Epoch 52/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1406 - acc: 0.9431 - val_loss: 0.4251 - val_acc: 0.9086\n",
      "Epoch 53/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1337 - acc: 0.9417 - val_loss: 0.4250 - val_acc: 0.9083\n",
      "Epoch 54/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.1437 - acc: 0.9405 - val_loss: 0.4246 - val_acc: 0.9077\n",
      "Epoch 55/500\n",
      "484/484 [==============================] - 0s 323us/step - loss: 0.1469 - acc: 0.9418 - val_loss: 0.4249 - val_acc: 0.9077\n",
      "Epoch 56/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.1427 - acc: 0.9421 - val_loss: 0.4243 - val_acc: 0.9071\n",
      "Epoch 57/500\n",
      "484/484 [==============================] - 0s 327us/step - loss: 0.1384 - acc: 0.9448 - val_loss: 0.4247 - val_acc: 0.9074\n",
      "Epoch 58/500\n",
      "484/484 [==============================] - 0s 322us/step - loss: 0.1441 - acc: 0.9435 - val_loss: 0.4237 - val_acc: 0.9074\n",
      "Epoch 59/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1377 - acc: 0.9409 - val_loss: 0.4237 - val_acc: 0.9077\n",
      "Epoch 60/500\n",
      "484/484 [==============================] - 0s 334us/step - loss: 0.1460 - acc: 0.9438 - val_loss: 0.4241 - val_acc: 0.9077\n",
      "Epoch 61/500\n",
      "484/484 [==============================] - 0s 330us/step - loss: 0.1379 - acc: 0.9446 - val_loss: 0.4243 - val_acc: 0.9071\n",
      "Epoch 62/500\n",
      "484/484 [==============================] - 0s 329us/step - loss: 0.1456 - acc: 0.9416 - val_loss: 0.4247 - val_acc: 0.9077\n",
      "Epoch 63/500\n",
      "484/484 [==============================] - 0s 330us/step - loss: 0.1389 - acc: 0.9420 - val_loss: 0.4248 - val_acc: 0.9077\n",
      "Epoch 64/500\n",
      "484/484 [==============================] - 0s 319us/step - loss: 0.1377 - acc: 0.9464 - val_loss: 0.4246 - val_acc: 0.9077\n",
      "Epoch 65/500\n",
      "484/484 [==============================] - 0s 335us/step - loss: 0.1420 - acc: 0.9428 - val_loss: 0.4230 - val_acc: 0.9074\n",
      "Epoch 66/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1395 - acc: 0.9468 - val_loss: 0.4232 - val_acc: 0.9077\n",
      "Epoch 67/500\n",
      "484/484 [==============================] - 0s 328us/step - loss: 0.1385 - acc: 0.9436 - val_loss: 0.4230 - val_acc: 0.9074\n",
      "Epoch 68/500\n",
      "484/484 [==============================] - 0s 332us/step - loss: 0.1352 - acc: 0.9469 - val_loss: 0.4236 - val_acc: 0.9074\n",
      "Epoch 69/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1438 - acc: 0.9439 - val_loss: 0.4238 - val_acc: 0.9077\n",
      "Epoch 70/500\n",
      "484/484 [==============================] - 0s 333us/step - loss: 0.1357 - acc: 0.9451 - val_loss: 0.4228 - val_acc: 0.9077\n",
      "Epoch 71/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1373 - acc: 0.9438 - val_loss: 0.4233 - val_acc: 0.9077\n",
      "Epoch 72/500\n",
      "484/484 [==============================] - 0s 326us/step - loss: 0.1283 - acc: 0.9509 - val_loss: 0.4229 - val_acc: 0.9083\n",
      "The 10th iteration is run\n"
     ]
    }
   ],
   "source": [
    "dcdr_model = Binning_CDF(num_cut=num_cut, hidden_list=hidden_list,\n",
    "                         histogram_bin=histogram_bin, dropout_list=dropout_list,\n",
    "                         seeding=seeding, loss_model=loss_model, \n",
    "                         niter=10)\n",
    "    \n",
    "dcdr_model.fit_cdf(inputs, targets, batch_size=32, merge_empty_bin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = dcdr_model.predict_cdf(X_RBF_pred,pred_lim=[12,12],ngrid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "riskdf=pd.DataFrame(risk)\n",
    "riskdf.to_csv('../../PMdata/risk_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
